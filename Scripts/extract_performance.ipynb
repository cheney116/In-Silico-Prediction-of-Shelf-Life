{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7806801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the score files\n",
    "score_files_dir = \"C:/Users/aim2r/Desktop/GUI/V1_code/metrics/\"\n",
    "\n",
    "# Function to extract metrics from each file\n",
    "def extract_metrics(file_path):\n",
    "    # Extract the machine learning method name from the filename\n",
    "    file_name = os.path.basename(file_path)\n",
    "    ml_method = file_name.replace(\"scores_\", \"\").replace(\".txt\", \"\")\n",
    "\n",
    "    metrics_values = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        current_method = None\n",
    "        for line in lines:\n",
    "            if \"Encoding method:\" in line:\n",
    "                # Extract the encoding method name\n",
    "                current_method = line.split(':')[1].strip().replace(\"_encoded.csv\", \"\")\n",
    "            elif current_method:  # Assuming similar structure for Accuracy, Precision, Recall, and F1 Score\n",
    "                parts = line.split(':')\n",
    "                metric_name = parts[0].strip()\n",
    "                if metric_name in ['Average Accuracy', 'Average Precision', 'Average Recall', 'Average F1 score', 'Average MCC']:  # Check if it's a relevant metric\n",
    "                    metric_value = float(parts[1].strip()) \n",
    "                    if current_method not in metrics_values:\n",
    "                        metrics_values[current_method] = {}\n",
    "                    if metric_name not in metrics_values[current_method]:\n",
    "                        metrics_values[current_method][metric_name] = []\n",
    "                    metrics_values[current_method][metric_name].append(metric_value)\n",
    "    return ml_method, metrics_values\n",
    "\n",
    "# List all score files in the directory that start with \"scores\"\n",
    "score_files = [file for file in os.listdir(score_files_dir) if file.startswith(\"scores\")]\n",
    "all_metrics = {}\n",
    "\n",
    "# Extract metrics from each file and collect them into a dictionary\n",
    "for file in score_files:\n",
    "    score_file_path = os.path.join(score_files_dir, file)\n",
    "    ml_method, metrics_values = extract_metrics(score_file_path)\n",
    "    for method, metrics in metrics_values.items():\n",
    "        if method not in all_metrics:\n",
    "            all_metrics[method] = {}\n",
    "        for metric_name, values in metrics.items():\n",
    "            if metric_name not in all_metrics[method]:\n",
    "                all_metrics[method][metric_name] = []\n",
    "            all_metrics[method][metric_name].extend([(ml_method, v) for v in values])\n",
    "\n",
    "# Convert the dictionary to a DataFrame for plotting\n",
    "data = []\n",
    "for method, metrics in all_metrics.items():\n",
    "    for metric_name, values in metrics.items():\n",
    "        for ml_method, value in values:\n",
    "            data.append({'ML Method': ml_method, 'Encoding Method': method, 'Metric': metric_name, 'Value': value})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('performance_all.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe5083a-b7e4-41c9-bd8e-fe7244df0794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
